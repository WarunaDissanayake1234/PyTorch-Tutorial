{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhFDxJesuhgrok8Xw31vQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WarunaDissanayake1234/PyTorch-Tutorial/blob/main/PyTorch_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word embedding model âˆ’ word2vec"
      ],
      "metadata": {
        "id": "LxquUqyyJUbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the libraries in word embedding"
      ],
      "metadata": {
        "id": "YYJZOTe5KWGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N2NlzBEJJQoX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Skip Gram Model of word embedding with the class called word2vec.\n",
        "\n",
        " It includes emb_size, emb_dimension, u_embedding, v_embedding"
      ],
      "metadata": {
        "id": "MEQpYH9rK8i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "   def __init__(self, emb_size, emb_dimension):\n",
        "      super(SkipGramModel, self).__init__()\n",
        "      self.emb_size = emb_size\n",
        "      self.emb_dimension = emb_dimension\n",
        "      self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "      self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse = True)\n",
        "      self.init_emb()\n",
        "   def init_emb(self):\n",
        "      initrange = 0.5 / self.emb_dimension\n",
        "      self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "      self.v_embeddings.weight.data.uniform_(-0, 0)\n",
        "   def forward(self, pos_u, pos_v, neg_v):\n",
        "      emb_u = self.u_embeddings(pos_u)\n",
        "      emb_v = self.v_embeddings(pos_v)\n",
        "      score = torch.mul(emb_u, emb_v).squeeze()\n",
        "      score = torch.sum(score, dim = 1)\n",
        "      score = F.logsigmoid(score)\n",
        "      neg_emb_v = self.v_embeddings(neg_v)\n",
        "      neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
        "      neg_score = F.logsigmoid(-1 * neg_score)\n",
        "      return -1 * (torch.sum(score)+torch.sum(neg_score))\n",
        "   def save_embedding(self, id2word, file_name, use_cuda):\n",
        "      if use_cuda:\n",
        "         embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "      else:\n",
        "         embedding = self.u_embeddings.weight.data.numpy()\n",
        "      fout = open(file_name, 'w')\n",
        "      fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "      for wid, w in id2word.items():\n",
        "         e = embedding[wid]\n",
        "         e = ' '.join(map(lambda x: str(x), e))\n",
        "         fout.write('%s %s\\n' % (w, e))\n",
        "def test():\n",
        "   model = SkipGramModel(100, 100)\n",
        "   id2word = dict()\n",
        "   for i in range(100):\n",
        "      id2word[i] = str(i)\n",
        "   model.save_embedding(id2word)"
      ],
      "metadata": {
        "id": "ws4B6mtPK6Vn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implement the main method to get the word embedding model displayed in proper way\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   test()\n"
      ],
      "metadata": {
        "id": "EFujuQC1NYtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}